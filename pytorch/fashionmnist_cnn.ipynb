{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dae524-aad5-4fe5-89dc-5b438a094c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4bee2d1-fdd9-4b71-b278-530da4ad31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bc6c9a-0d8e-41ee-8626-6bf677373903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccecd37c-afff-403f-9970-629d6cc52336",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "training_loader =DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021b3841-4208-4a62-91eb-bc3578f6f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba66625-52d5-4691-bae7-9892b8553b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 28, 28]), tensor([2, 6, 4, 7, 3, 4, 2, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f8cabc-668c-4ee4-a4a2-475320248a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fd7148-ee2e-4858-b6aa-8c5133919e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_CNN(nn.Module):\n",
    "    \"\"\"Custom CNN class for image classification\"\"\"\n",
    "    def __init__(self, n_classes = 10):\n",
    "        \"\"\"constructor for initialisation\"\"\"\n",
    "        super().__init__()\n",
    "        \"\"\"initialise parents method first so we can use methods from parent class without overriding\"\"\"\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1), # [B, 16, 28, 28]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 32,kernel_size=3, padding=1), # [B, 32, 28, 28]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size= 3 , padding=1), #[B, 64, 28, 28]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), #[B, 64, 14, 14]\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # [B, 128, 14, 14]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), # [B, 128, 14, 14]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2), #[B, 128, 7, 7]\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), # [B, 128]\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view((x.shape[0], -1))\n",
    "        # print(x.shape)\n",
    "        x = self.linear_layer(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06ae5798-17cd-4523-bc2d-bbfa89c2027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932a23cf-7150-4fdd-807f-4bf893f1850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Custom_CNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f35bead5-3016-4818-8b07-e593fba79aad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd5d91e1-636c-437e-acc9-e77751a42af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:46<00:00, 45.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, average loss: 2.302831979115804\n",
      "evaluation accuracy: 0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:46<00:00, 45.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, average loss: 2.301485323047638\n",
      "evaluation accuracy: 0.1629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:45<00:00, 45.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, average loss: 2.2917790589968363\n",
      "evaluation accuracy: 0.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:47<00:00, 44.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, average loss: 2.158912260421117\n",
      "evaluation accuracy: 0.3387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:45<00:00, 45.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, average loss: 1.4575017652710278\n",
      "evaluation accuracy: 0.6396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:47<00:00, 44.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, average loss: 0.9358826000948747\n",
      "evaluation accuracy: 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:47<00:00, 44.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, average loss: 0.7796953303277493\n",
      "evaluation accuracy: 0.7342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:49<00:00, 44.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, average loss: 0.6955820003921787\n",
      "evaluation accuracy: 0.7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:47<00:00, 44.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, average loss: 0.6438950349072615\n",
      "evaluation accuracy: 0.7774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [02:50<00:00, 44.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, average loss: 0.6041845050953328\n",
      "evaluation accuracy: 0.7903\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0\n",
    "    for img, label in tqdm(training_loader):\n",
    "        # forward pass, comput output tensors from input tensors\n",
    "        y_pred = model(img)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(y_pred, label)\n",
    "\n",
    "        # zero gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # back propagation to compute gradients with respect to loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameter\n",
    "        optimizer.step()\n",
    "            \n",
    "        running_loss+= loss.item()\n",
    "    print(f\"epoch: {epoch + 1}, average loss: {running_loss/len(training_loader)}\")\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for img, label in test_loader:\n",
    "        outputs = model(img)\n",
    "        y_pred = outputs.argmax(dim = 1)\n",
    "        correct += (y_pred == label).sum().item()\n",
    "        total += len(y_pred)\n",
    "\n",
    "    print(f\"evaluation accuracy: {correct / total}\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2ad6bab5-a79f-4010-910a-92b22067efa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9d3a0eaf-8b91-44c3-9c44-b88c19596e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c28e363-4d65-4734-aa06-5d79e7ecee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 5, 4, 0])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6d85c9c4-e7b5-47ef-8f66-b50933eef827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(1) == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b50b3dfe-3fba-4211-b18f-a01bb9edd087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2750b8bc-7c0f-4c59-b0c9-389d6da41855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred.argmax(1) == label).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e311dee-1b03-4494-a748-aa54b26c0c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3402626514434814"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3e9d2-c9ba-4b79-82d0-038dc588ea1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
